# LLM Qur'an Translations & Interpretations

A Streamlit web application that displays and compares Qur'an translations and interpretations generated by 15 different Large Language Models.

## Data source

All translation data is fetched at runtime from:
[jsnhum/LLM.-Qur-an-translation](https://github.com/jsnhum/LLM.-Qur-an-translation)

### Models included

| Family | Models |
|--------|--------|
| Anthropic Claude | Haiku, Opus, Sonnet, Sonnet 3.5 |
| OpenAI GPT | GPT-3, GPT-4 Turbo, GPT-4o, GPT-4o Mini |
| Google Gemini | 1.5 Flash, 1.5 Pro, 2.0 Flash |
| Meta Llama | Llama 2, Llama 3 |
| xAI | Grok 2 |
| Mistral | Mixtral |

## Run locally

```bash
pip install -r requirements.txt
streamlit run quran_interpretations_app.py
```

## Deploy to Streamlit Community Cloud

1. Push this repository to GitHub.
2. Go to [share.streamlit.io](https://share.streamlit.io) and sign in with GitHub.
3. Click **New app** and select:
   - **Repository:** your GitHub repo
   - **Branch:** `main`
   - **Main file path:** `quran_interpretations_app.py`
4. Click **Deploy**. The app will install dependencies from `requirements.txt` automatically.

## Project structure

```
├── quran_interpretations_app.py   # Main Streamlit application
├── requirements.txt               # Python dependencies
├── README.md                      # This file
└── .streamlit/
    └── config.toml                # Theme and server configuration
```
